"""
ê°„ë‹¨í•œ KoBERT ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path

class SimpleEmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

def main():
    print("ğŸš€ ê°„ë‹¨í•œ KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    
    # ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, 'data', 'labeled', 'auto_labeled_data.csv')
    df = pd.read_csv(data_path)
    print(f'ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(df)}ê°œ')
    
    # ê°ì • ë¼ë²¨ ë§¤í•‘
    emotion_to_label = {
        "ì¤‘ë¦½": 0, "ê¸°ì¨": 1, "ìŠ¬í””": 2, "ë¶„ë…¸": 3,
        "ë‘ë ¤ì›€": 4, "ë†€ë¼ì›€": 5, "í˜ì˜¤": 6
    }
    
    # ë°ì´í„° ì¤€ë¹„
    texts = df['text'].tolist()
    labels = [emotion_to_label.get(emotion, 0) for emotion in df['emotion']]
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´)
    texts = [text[:200] for text in texts]
    
    print(f"ğŸ“ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©)
    model_name = "klue/bert-base"
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)
    
    # Train/Test ë¶„ë¦¬
    train_texts, test_texts, train_labels, test_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42
    )
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SimpleEmotionDataset(train_texts, train_labels, tokenizer)
    test_dataset = SimpleEmotionDataset(test_texts, test_labels, tokenizer)
    
    print(f"ğŸ‹ï¸ í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ")
    
    # í•™ìŠµ ì„¤ì • (ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ê°„ì†Œí™”)
    training_args = TrainingArguments(
        output_dir='../models/kobert_emotion_simple',
        num_train_epochs=2,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 2 ì—í¬í¬ë§Œ
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir='../models/kobert_emotion_simple/logs',
        logging_steps=10,
        eval_strategy="steps",  # evaluation_strategy ëŒ€ì‹  eval_strategy ì‚¬ìš©
        eval_steps=50,
        save_strategy="epoch",
        load_best_model_at_end=True
    )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer
    )
    
    print("ğŸ¯ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    output_dir = Path('../models/kobert_emotion_simple')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {output_dir}")
    print("ğŸ‰ KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()