"""
Google Colabì—ì„œ KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
ì´ ì½”ë“œë¥¼ Colabì— ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

# ===== Google Colab ë…¸íŠ¸ë¶ ì½”ë“œ =====

# 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install transformers torch datasets scikit-learn pandas numpy

# 2. Google Drive ë§ˆìš´íŠ¸ (ëª¨ë¸ ì €ì¥ì„ ìœ„í•´)
from google.colab import drive
drive.mount('/content/drive')

# 3. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import numpy as np
from datetime import datetime

# 4. ê°ì • ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 5. ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œë¡œëŠ” ì—…ë¡œë“œí•œ CSV ì‚¬ìš©)
# CSV íŒŒì¼ì„ Colabì— ì—…ë¡œë“œí•˜ê³  ê²½ë¡œ ìˆ˜ì •
sample_data = {
    'text': [
        'ì˜¤ëŠ˜ ì •ë§ ê¸°ë»¤ì–´!',
        'ë„ˆë¬´ ìŠ¬í”„ë‹¤...',
        'í™”ê°€ ë‚˜ì„œ ë¯¸ì¹˜ê² ì–´',
        'ë¬´ì„œì›Œì„œ ë–¨ë ¤',
        'ì™€! ë†€ëë‹¤!',
        'ì •ë§ ì—­ê²¨ì›Œ',
        'ê·¸ëƒ¥ í‰ë²”í•œ í•˜ë£¨ì˜€ë‹¤'
    ] * 50,  # 350ê°œ ìƒ˜í”Œ
    'emotion': [
        'ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸', 'ë‘ë ¤ì›€', 
        'ë†€ë¼ì›€', 'í˜ì˜¤', 'ì¤‘ë¦½'
    ] * 50
}

df = pd.DataFrame(sample_data)
print(f"ë°ì´í„° í¬ê¸°: {len(df)}")
print(f"ê°ì • ë¶„í¬:\n{df['emotion'].value_counts()}")

# 6. ê°ì • ë¼ë²¨ ë§¤í•‘
emotion_to_label = {
    "ì¤‘ë¦½": 0, "ê¸°ì¨": 1, "ìŠ¬í””": 2, "ë¶„ë…¸": 3,
    "ë‘ë ¤ì›€": 4, "ë†€ë¼ì›€": 5, "í˜ì˜¤": 6
}

df['label'] = df['emotion'].map(emotion_to_label)

# 7. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)

# 8. ë°ì´í„° ë¶„í• 
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), 
    df['label'].tolist(), 
    test_size=0.2, 
    random_state=42,
    stratify=df['label']
)

# 9. ë°ì´í„°ì…‹ ìƒì„±
train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)
val_dataset = EmotionDataset(val_texts, val_labels, tokenizer)

print(f"í•™ìŠµ ë°ì´í„°: {len(train_dataset)}")
print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}")

# 10. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir='/content/drive/MyDrive/kobert_emotion_results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='/content/drive/MyDrive/kobert_emotion_logs',
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    report_to=None,
    dataloader_num_workers=2,
    fp16=True  # GPU ê°€ì†
)

# 11. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

# 12. í•™ìŠµ ì‹¤í–‰
print("ğŸš€ KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
trainer.train()

# 13. ìµœì¢… ëª¨ë¸ ì €ì¥
save_path = "/content/drive/MyDrive/kobert_emotion_final"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)

print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

# 14. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
    
    label_to_emotion = {v: k for k, v in emotion_to_label.items()}
    return label_to_emotion[predicted_class], predictions[0][predicted_class].item()

# 15. í…ŒìŠ¤íŠ¸
test_texts = [
    "ì˜¤ëŠ˜ ë„ˆë¬´ í–‰ë³µí•´!",
    "ì •ë§ í™”ê°€ ë‚˜ë„¤",
    "ìŠ¬í”ˆ ì¼ì´ ìƒê²¼ì–´",
    "ê·¸ëƒ¥ í‰ë²”í•œ í•˜ë£¨"
]

print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
for text in test_texts:
    emotion, confidence = predict_emotion(text)
    print(f"í…ìŠ¤íŠ¸: '{text}' â†’ ê°ì •: {emotion} (ì‹ ë¢°ë„: {confidence:.3f})")

# 16. ëª¨ë¸ íŒŒì¼ì„ ZIPìœ¼ë¡œ ì••ì¶• (ë‹¤ìš´ë¡œë“œ í¸ì˜ì„±)
!cd "/content/drive/MyDrive" && zip -r kobert_emotion_final.zip kobert_emotion_final/

print(f"""
ğŸ‰ í•™ìŠµ ì™„ë£Œ!

ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:
- ëª¨ë¸: /content/drive/MyDrive/kobert_emotion_final/
- ZIP: /content/drive/MyDrive/kobert_emotion_final.zip

ğŸ“± ë‹¤ìŒ ë‹¨ê³„:
1. Google Driveì—ì„œ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ
2. ë¡œì»¬ í”„ë¡œì íŠ¸ì˜ models/ í´ë”ì— ì••ì¶• í•´ì œ
3. FastAPI ì„œë¹„ìŠ¤ì—ì„œ ëª¨ë¸ ë¡œë“œ
4. ì›¹ì—ì„œ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ ì œê³µ

ğŸ”— ì—°ê²° ë°©ë²•ì€ COLAB_INTEGRATION_GUIDE.md ì°¸ì¡°
""")